{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f86e1b8",
   "metadata": {},
   "source": [
    "## Web scraping en ingl√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea26357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the website\n",
    "url = 'https://www.goodhousekeeping.com/home/craft-ideas/g1389/diy-kids-activities/'\n",
    "\n",
    "# Send a GET request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # List to hold the data\n",
    "    activities = []\n",
    "\n",
    "    # Find all activity blocks\n",
    "    articles = soup.find_all('div', class_='simple-item')\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h2')\n",
    "        description = article.find('p')\n",
    "\n",
    "        activity_data = {\n",
    "            'activity': title.get_text(strip=True) if title else None,\n",
    "            'description': description.get_text(strip=True) if description else None,\n",
    "            'age': None,               # Will try to extract later if possible\n",
    "            'how_many_people': None,   # Will try to infer\n",
    "            'price': None              # Will try to infer\n",
    "        }\n",
    "        \n",
    "        activities.append(activity_data)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(activities)\n",
    "    \n",
    "    # Show the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print('Failed to retrieve the page:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30dcb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL\n",
    "url = 'https://www.goodhousekeeping.com/home/craft-ideas/g1389/diy-kids-activities/'\n",
    "\n",
    "# Headers to avoid getting blocked\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# Get the page\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all slides (each activity)\n",
    "slides = soup.find_all('div', class_='slide')\n",
    "\n",
    "activities = []\n",
    "\n",
    "for slide in slides:\n",
    "    # Extract title\n",
    "    title_tag = slide.find(['h2', 'h3'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else None\n",
    "    \n",
    "    # Extract description\n",
    "    description_tag = slide.find('p')\n",
    "    description = description_tag.get_text(strip=True) if description_tag else None\n",
    "    \n",
    "    # Append to the list\n",
    "    activities.append({\n",
    "        'activity': title,\n",
    "        'description': description,\n",
    "        'age': None,               # To infer later\n",
    "        'how_many_people': None,   # To infer later\n",
    "        'price': None              # To infer later\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(activities)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASH\n",
    "#pip install selenium pandas webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc6e545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # run in background\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the page\n",
    "url = 'https://www.goodhousekeeping.com/home/craft-ideas/g1389/diy-kids-activities/'\n",
    "driver.get(url)\n",
    "\n",
    "# Give it time to load JavaScript\n",
    "time.sleep(5)\n",
    "\n",
    "# Find all slides (each activity)\n",
    "slides = driver.find_elements(By.CLASS_NAME, 'slide')\n",
    "\n",
    "activities = []\n",
    "\n",
    "for slide in slides:\n",
    "    # Extract title\n",
    "    try:\n",
    "        title = slide.find_element(By.TAG_NAME, 'h2').text\n",
    "    except:\n",
    "        try:\n",
    "            title = slide.find_element(By.TAG_NAME, 'h3').text\n",
    "        except:\n",
    "            title = None\n",
    "    \n",
    "    # Extract description\n",
    "    try:\n",
    "        description = slide.find_element(By.TAG_NAME, 'p').text\n",
    "    except:\n",
    "        description = None\n",
    "    \n",
    "    activities.append({\n",
    "        'activity': title,\n",
    "        'description': description,\n",
    "        'age': None,\n",
    "        'how_many_people': None,\n",
    "        'price': None\n",
    "    })\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(activities)\n",
    "\n",
    "# Show\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32f3f0",
   "metadata": {},
   "source": [
    "### https://indyschild.com/70-things-to-do-with-kids-now-that-were-all-stuck-at-home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae7008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the webpage content\n",
    "url = 'https://indyschild.com/70-things-to-do-with-kids-now-that-were-all-stuck-at-home/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Step 3: Extract activities\n",
    "    # Find the main content area\n",
    "    content_div = soup.find('div', class_='entry-content')\n",
    "    activities = []\n",
    "\n",
    "    if content_div:\n",
    "        # Find all list items within the content\n",
    "        list_items = content_div.find_all('li')\n",
    "        for item in list_items:\n",
    "            activity_text = item.get_text(strip=True)\n",
    "            activities.append({\n",
    "                'activity': activity_text,\n",
    "                'age': None,\n",
    "                'how_many_people': None,\n",
    "                'price': None\n",
    "            })\n",
    "\n",
    "    # Step 4: Create a DataFrame\n",
    "    df = pd.DataFrame(activities)\n",
    "\n",
    "    # Display the first few rows\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23594fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'https://indyschild.com/70-things-to-do-with-kids-now-that-were-all-stuck-at-home/'\n",
    "MAIN_URL = BASE_URL + \"/madrid/es/que-hacer\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "def get_event_links():\n",
    "    res = requests.get(MAIN_URL, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    cards = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Filtrar solo URLs de eventos\n",
    "    event_links = [BASE_URL + a[\"href\"] for a in cards if \"/madrid/es/que-hacer/\" in a[\"href\"]]\n",
    "    return list(set(event_links))  # Eliminar duplicados\n",
    "\n",
    "def parse_event(url):\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    def safe_text(selector):\n",
    "        tag = soup.select_one(selector)\n",
    "        return tag.get_text(strip=True) if tag else \"N/A\"\n",
    "\n",
    "    nombre_evento = safe_text(\"h1, h2\")\n",
    "    descripcion = soup.get_text(\" \", strip=True).lower()\n",
    "\n",
    "    # Reglas b√°sicas por palabras clave\n",
    "    discapacidad = \"visual\" if \"lengua de signos\" in descripcion else \"ninguna\"\n",
    "    modalidad = \"exterior\" if \"aire libre\" in descripcion else \"interior\"\n",
    "    costo = \"gratis\" if \"gratis\" in descripcion else \"pago\"\n",
    "\n",
    "    # Valores manuales o heur√≠sticos\n",
    "    edad_dirigida = \"todas las edades\" if \"familiar\" in descripcion or \"ni√±os\" in descripcion else \"N/A\"\n",
    "    min_integrantes = \"N/A\"\n",
    "    ubicacion = safe_text(\"[data-testid*=location]\") or \"N/A\"\n",
    "    categoria = safe_text(\"ul.breadcrumbs li:nth-last-child(1)\") or \"N/A\"\n",
    "\n",
    "    return {\n",
    "        \"nombre_evento\": nombre_evento,\n",
    "        \"categor√≠a\": categoria,\n",
    "        \"discapacidad\": discapacidad,\n",
    "        \"ubicaci√≥n\": ubicacion,\n",
    "        \"costo\": costo,\n",
    "        \"edad_dirigida\": edad_dirigida,\n",
    "        \"min_integrantes\": min_integrantes,\n",
    "        \"modalidad\": modalidad,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "# Extraer eventos\n",
    "eventos = []\n",
    "for link in get_event_links()[:10]: \n",
    "    try:\n",
    "        print(f\"Procesando: {link}\")\n",
    "        evento = parse_event(link)\n",
    "        eventos.append(evento)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {link}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(eventos)\n",
    "\n",
    "df\n",
    "\n",
    "# Opcional: Guardar en CSV\n",
    "#df.to_csv(\"eventos_timeout_madrid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715b616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
